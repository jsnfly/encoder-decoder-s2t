{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3624a95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary to disable warnings.\n",
    "%env TOKENIZERS_PARALLELISM=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd525fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from accelerate import Accelerator\n",
    "from transformers import AutoTokenizer, Wav2Vec2FeatureExtractor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim import AdamW, lr_scheduler\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from cv8_en import prepare\n",
    "from model.wav2vec_gpt2 import Wav2VecGPT2Model\n",
    "from wer import calculate_wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65922922",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator(fp16=True)\n",
    "print(f'Using {accelerator.device}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc43f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = Path('./results/0')\n",
    "LOG_PATH = OUTPUT_PATH / 'logs'\n",
    "\n",
    "SAMPLING_RATE = 16_000\n",
    "SEED = 419\n",
    "USE_TRAIN_PCT = 0.1\n",
    "USE_VAL_PCT = 0.05\n",
    "\n",
    "ENCODER_ID = 'facebook/wav2vec2-base-960h'\n",
    "DECODER_ID = 'gpt2'\n",
    "PROMPT = 'Transcription:'\n",
    "PAD_TOKEN = '_'\n",
    "MAX_AUDIO_LENGTH = 300_000\n",
    "MAX_TOKEN_SEQ_LEN = 39\n",
    "\n",
    "LEARNING_RATE = 3e-4\n",
    "BATCH_SIZE = 1\n",
    "MAX_EPOCHS=4\n",
    "ACCUMULATE_GRAD=8\n",
    "\n",
    "def LR_SCHEDULER(optimizer):\n",
    "    num_steps = MAX_EPOCHS * (len(train_ds) // (BATCH_SIZE * ACCUMULATE_GRAD)) * 1.1\n",
    "    return lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_steps, eta_min=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da84467",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, uncommon_chars = prepare('train', USE_TRAIN_PCT, SAMPLING_RATE, SEED)\n",
    "val, _ = prepare('validation', USE_VAL_PCT, SAMPLING_RATE, SEED, uncommon_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a97e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(ENCODER_ID)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(DECODER_ID)\n",
    "tokenizer.add_special_tokens({'pad_token': PAD_TOKEN})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86112c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, ds, prompt):\n",
    "        self.ds = ds\n",
    "        self.prompt = prompt + ' '\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        eg = self.ds[idx]\n",
    "        \n",
    "        # TODO: Do this somewhere else?\n",
    "        eg['sentence'] = self.prompt + eg['sentence']\n",
    "        return eg['audio']['array'], eg['sentence']\n",
    "    \n",
    "def collate_fn(examples):\n",
    "    # Remove the longest examples, these may lead to OOM-Errors.\n",
    "    examples = [eg for eg in examples if len(eg[0]) < MAX_AUDIO_LENGTH]\n",
    "    \n",
    "    audio_features = feature_extractor(\n",
    "        [eg[0] for eg in examples], sampling_rate=16_000, return_tensors='pt', padding='longest'\n",
    "    ).input_values\n",
    "    \n",
    "    input_ids = tokenizer(\n",
    "        [eg[1] for eg in examples], return_tensors='pt', padding=True\n",
    "    ).input_ids\n",
    "    \n",
    "    return audio_features, input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be878f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = AudioDataset(train, PROMPT)\n",
    "val_ds = AudioDataset(val, PROMPT)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=4)\n",
    "val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0e8fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Wav2VecGPT2Model.from_encoder_decoder_pretrained(ENCODER_ID, DECODER_ID)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac6b14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.0)\n",
    "lr_scheduler = LR_SCHEDULER(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48cfd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, train_dl, val_dl = accelerator.prepare(model, optimizer, train_dl, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27bde91",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(LOG_PATH)\n",
    "val_golds = [eg[1][len(PROMPT) + 1:] for eg in val_ds]\n",
    "global_train_step, val_count = 0, 0\n",
    "prompt_token_count = len(tokenizer(PROMPT).input_ids)\n",
    "best_wer = 10.\n",
    "\n",
    "def evaluate():\n",
    "    global val_count, best_wer\n",
    "    \n",
    "    model.eval()\n",
    "    val_preds = []\n",
    "    for audio_features, input_ids in val_dl:\n",
    "        with torch.no_grad():\n",
    "            generated = model.generate(\n",
    "                audio_features,\n",
    "                decoder_input_ids=input_ids[:, :prompt_token_count],\n",
    "                max_length=MAX_LEN\n",
    "            )\n",
    "        val_preds += tokenizer.batch_decode(generated)\n",
    "    val_preds = [pred[len(PROMPT) + 1:].rstrip(PAD_TOKEN) for pred in val_preds]\n",
    "    wer = calculate_wer(val_preds, val_golds)\n",
    "    writer.add_scalar('val_wer', wer, val_count)\n",
    "    print('WER: ', wer)\n",
    "\n",
    "    if wer < best_wer:\n",
    "        best_wer = wer\n",
    "        model.save_pretrained(OUTPUT_PATH)\n",
    "        print('Saved new best model.')\n",
    "    val_count += 1\n",
    "    return val_preds\n",
    "\n",
    "\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    model.train()\n",
    "    for audio_features, input_ids in train_dl:\n",
    "        print('i')\n",
    "        global_train_step += 1\n",
    "        out = model(audio_features,\n",
    "                    decoder_input_ids=input_ids[:, :-1], \n",
    "                    labels=input_ids[:, 1:].contiguous())\n",
    "        accelerator.backward(out.loss)\n",
    "        writer.add_scalar('train_loss', out.loss.item(), global_train_step)\n",
    "        [writer.add_scalar(f'learning_rate_group{i}', group['lr'], global_train_step) \n",
    "         for i, group in enumerate(optimizer.param_groups)]\n",
    "        \n",
    "        if (global_train_step + 1) % ACCUMULATE_GRAD == 0:\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        if global_train_step % 20 == 0:\n",
    "            print(out.loss.item())\n",
    "        \n",
    "        if global_train_step % 5000 == 0:\n",
    "            val_preds = evaluate()\n",
    "            model.train()\n",
    "            \n",
    "# Final evaluation.\n",
    "val_preds = evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bf72c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_golds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9fb01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ecda7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ./train.ipynb {LOG_PATH}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
